# 优化器
管理并更新模型中可学习参数的值，使得模型输出更接近真实标签
## 基本属性
* defaults: 优化器超参数
* state：参数的缓存，如momentum的缓存
* param_groups: 管理的参数组
* _step_count：记录更新次数，学习率调整中使用

## 基本方法：
* zero_grad()：清空所管理参数的梯度
* step（）：执行一步更新
* add_param_group()：添加参数组

      w2 = torch.randn((3,3),requires_grad=True)
      optimiter.add_param_group({"params":w2, 'lr':0.0001})

* state_dict()：获取优化器当前状态信息字典
* load_state_dict():加载状态信息字典

# 学习率调整策略（0.01,0.001,0.0001）
## class _LRScheduler
主要属性
* optimizer：关联的优化器
* last_epoch: 记录epoch数
* base_lrs:记录初始学习率
* step（）：更新下一个epoch的学习率（放到epoch的循环当中）
* get_lr():虚函数，计算下一个epoch的学习率

## StepLR
功能：等间隔调整学习率
主要参数：
* step_szie:调整间隔数（每隔n个epoch进行一次调整）
* gamma：调整系数
调整方式：lr = lr *  gamma

## MultiStepLR
功能：按给定间隔调整学习率
主要参数：
* milestones:设定调整时刻数（定义列表）
* gamma：调整系数
* 调整方式：lr = lr *  gamma 
scheduler_lr = optim.lr_scheduler.MultiStepLR(optimizer,milestones=milestones,gamma=0.1)

## ExponentialLR
功能：按指数衰减调整学习率
主要参数：
* gamma：指数的底
* 调整方式：lr = lr *  gamma ** epoch

## CosineAnnealingLR
功能：余弦周期调整学习率
主要参数：
* T_max:下降周期
* eta_min：学习率下限

## ReduceLRonPlateau
功能：监控指标，当指标不再变化则调整
主要参数：
* model：min/max 两种模式
* factor:调整系数
* patience：“耐心”，接收几次不变化
* cooldown:“冷却时间”，停止监控一段时间
* verbose：是否打印日志
* min_lr:学习率下限
* eps：学习率衰减最小值

## LambdaLR
功能：自定义调整策略
主要参数：
* lr_lambda：function or list
 
